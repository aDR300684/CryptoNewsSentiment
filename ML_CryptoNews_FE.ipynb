{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d565dafc-7bb0-417c-818a-057c63d22164",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In this notebook, we are going to perform feature engineering on the news articles. Our goal is to uncover latent topics within these articles using topic modeling techniques, specifically TF-IDF and LDA (Latent Dirichlet Allocation). This process will help us identify prevalent themes and potentially enhance our predictive model's ability to forecast cryptocurrency price momentum. After integrating sentiment scores into our dataset, we will explore correlation analysis to identify relationships between article sentiments, features, and price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32d547f1-4223-437f-8c3f-f3c5c2714224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset loaded. Here are the first rows:\n",
      "\n",
      "     datetime                                               text  \\\n",
      "0  2022-10-14  despite fact blockchainbased carbon credit mar...   \n",
      "1  2022-10-14  trader gained huge kudos space predicting drop...   \n",
      "2  2022-10-14  always worked sticking plan clear invalidation...   \n",
      "3  2022-10-14  fact broke level system giving bullish signals...   \n",
      "4  2022-10-14  demand coming confirms theres fuel keep going ...   \n",
      "\n",
      "                                                 url  price_momentums  \n",
      "0  https://cryptonews.com/news/bitcoin-price-and-...                1  \n",
      "1  https://cryptonews.com/news/bitcoin-price-pred...                1  \n",
      "2  https://cryptonews.com/news/bitcoin-price-pred...                1  \n",
      "3  https://cryptonews.com/news/bitcoin-price-pred...                1  \n",
      "4  https://cryptonews.com/news/bitcoin-price-pred...                1  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Base path for datasets\n",
    "dataset_path = \"C:/Users/adrco/Final_Project-env/Datasets/\"\n",
    "\n",
    "# Path for the combined dataset\n",
    "combined_file = dataset_path + \"full_set.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(combined_file)\n",
    "\n",
    "# Display the first rows \n",
    "print(f\"Combined dataset loaded. Here are the first rows:\\n\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f4793-7676-4013-ac9b-5c93b0747a03",
   "metadata": {},
   "source": [
    "## Text Preprocessing\r\n",
    "\r\n",
    "Upon initial observation, the `text` column in our dataset seems to have undergone some preprocessing steps already, such as removal of certain characters and lowercasing. However, for the sake of thoroughness and to ensure our analysis pipeline is robust and scalable, we will outline a comprehensive preprocessing step here. \r\n",
    "\r\n",
    "This is an essential practice to make our model more adaptable, especially when integrating new, unprocessed text data in the future. It ensures consistency in text handling and can improve the model's performance and accuracy. Our preprocessing will include converting text to lowercase, removing punctuation and special characters, tokenization, and optionally removing stopwords.\r\n",
    "\r\n",
    "This step, although might seem redundant for this specific dataset, sets a precedent for handling raw text data that may be introduced into our system at a later stage.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fbc3340-dd13-40a9-822b-e792dab41735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adrco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adrco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Adjusting the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits, keeping spaces\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)  \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Join the tokens back into a string with spaces\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to each article's text\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b401c0a0-2b9e-4e3b-80db-2c8bf53c1de7",
   "metadata": {},
   "source": [
    "## Applying TF-IDF\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects how important a word is to a document in a collection or corpus. We'll use it to convert our textual data into a format that's suitable for topic modeling, focusing on the `processed_text` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41eab6b2-ab69-4a8a-90b8-ab1ff7f804f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (8113, 571)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=50, stop_words='english')\n",
    "\n",
    "# Apply TF-IDF to the processed text\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# View shape of TF-IDF matrix\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634bab7d-0eba-40f8-844d-a5eab64aa918",
   "metadata": {},
   "source": [
    "### Interpretation of TF-IDF Matrix Shape\n",
    "\n",
    "The TF-IDF matrix has a shape of (8113, 571). This indicates that our dataset, consisting of 8113 cryptocurrency news articles, has been distilled to 571 unique and significant terms or words. Each row in this matrix corresponds to an individual document, and each column represents a unique term identified across the corpus. The values within the matrix reflect the term's importance or weight in each document, adjusted by the term's frequency across all documents. This refined representation is pivotal for our analysis, as it ensures that the vocabulary size is manageable yet sufficiently comprehensive to capture the essence of the discussions within our dataset. It forms a solid foundation for performing nuanced topic modeling, enabling us to extract meaningful insights and themes from the cryptocurrency news landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6f4ec-c97c-4ee3-a130-42895efd366a",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA) for Topic Modeling\n",
    "\n",
    "LDA is a type of statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For topic modeling, it helps in discovering the topics that pervade through the cryptocurrency news articles, based on the TF-IDF transformed `processed_text` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8395a3f-869e-4faa-8b02-1c37f6b081e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 5 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 6 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 7 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 8 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 9 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 10 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 11 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 12 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 13 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 14 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 15 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 16 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 17 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 18 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 19 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 20 of max_iter: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "bank news central crypto currency country stated president claims legal\n",
      "Topic 2:\n",
      "bitcoin btc high number miners addresses transactions time mining 2021\n",
      "Topic 3:\n",
      "saying added tax business claimed explained sector used impt public\n",
      "Topic 4:\n",
      "digital bitcoin company investment mining report asset assets firm services\n",
      "Topic 5:\n",
      "24 hours value price day market cryptocurrencies increased dropped trade\n",
      "Topic 6:\n",
      "fed rate inflation rates federal reserve markets data expected year\n",
      "Topic 7:\n",
      "level support bullish resistance bearish break moving btc trend price\n",
      "Topic 8:\n",
      "crypto said south police money government korean exchange companies securities\n",
      "Topic 9:\n",
      "billion price volume prediction ethereum bitcoin btc trading 24hour current\n",
      "Topic 10:\n",
      "ftx exchange crypto collapse binance ceo look bankruptcy volume lets\n",
      "Topic 11:\n",
      "bitcoin market price bull bear bitcoins onchain indicators btc volatility\n",
      "Topic 12:\n",
      "bitcoin gold area price btc long key drop short dollar\n",
      "Topic 13:\n",
      "presale million stage token raised tokens d2t el usdt heres\n",
      "Topic 14:\n",
      "blockchain users bitcoin activity tweet twitter technology network future development\n",
      "Topic 15:\n",
      "platform trading trade make world states social dash traders intelligence\n"
     ]
    }
   ],
   "source": [
    "# Initialize LDA Model\n",
    "lda_model = LatentDirichletAllocation(n_components=15, learning_decay=0.7, random_state=42, verbose=10, max_iter=20)\n",
    "\n",
    "\n",
    "# Fit LDA model to the TF-IDF matrix\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx + 1}:\")  # Adjust topic index for display\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# Displaying the topics\n",
    "display_topics(lda_model, tfidf_vectorizer.get_feature_names_out(), 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1addb080-d423-4b03-98a6-1ef258eaf80c",
   "metadata": {},
   "source": [
    "## Optimizing Topic Modeling \n",
    "\n",
    "After extensive experimentation with various parameters for TF-IDF vectorization and Latent Dirichlet Allocation (LDA) modeling, we have identified a configuration that appears to be well-suited for our project needs. Our goal was to uncover meaningful and distinct topics within a large dataset of cryptocurrency news articles, which required a careful balance between the breadth of vocabulary and the depth of topics extracted.\n",
    "\n",
    "### Final Parameters\n",
    "\n",
    "The final parameters that yielded the most relevant and interpretable topics were as follows:\n",
    "- **TF-IDF Vectorization**: `max_df=0.90` and `min_df=50`, with English stop words excluded. This setup produced a TF-IDF matrix with a shape of (8113, 571), indicating that our corpus of 8113 documents was distilled down to 571 significant terms. These parameters helped in focusing on words that are prevalent enough to be informative but not so common as to be ubiquitous across documents.\n",
    "- **LDA Modeling**: We chose `n_components=15` to identify a diverse range of topics, with a `learning_decay` of 0.7 to optimize the learning rate, and `max_iter=20` to ensure convergence. The `random_state` was set to 42 for reproducibility, and `verbose=10` for detailed logging during the model fitting process.\n",
    "\n",
    "### Interpretation of Model Outputs\n",
    "\n",
    "The LDA model revealed 15 distinct topics, each representing a unique aspect of the cryptocurrency domain. We can groups them into 5 overarching themes based on their content:\n",
    "\n",
    "- **Topics Overview**:\n",
    "  - **Regulatory and Legal Aspects**: Topics like banking regulations and legal claims within the crypto space (Topic 1).\n",
    "  - **Technical and Market Analysis**: Deep dives into bitcoin mining, transaction analysis, and market movements (Topics 2, 5, 7, 11).\n",
    "  - **Economic Factors**: Examination of federal rates, inflation, and their impact on cryptocurrencies (Topic 6).\n",
    "  - **Specific Events and Sectors**: Insight into significant events like the FTX collapse (Topic 10) and focuses on sectors such as digital asset investment (Topic 3).\n",
    "  - **Innovation and Development**: Discussions on blockchain technology and its future developments (Topic 14).\n",
    "\n",
    "The topics identified provide a comprehensive overview of the cryptocurrency news landscape, from economic and regulatory discussions to technological advancements and market analysis.\n",
    "\n",
    "\n",
    "The chosen parameters for TF-IDF vectorization and LDA modeling have successfully facilitated the extraction of meaningful topics from our cryptocurrency news dataset. The resulting topics are not only interpretable and relevant but also cover a broad spectrum of discussions within the cryptocurrency domain, making them invaluable for further analysis and insights generation in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a7b120-e5ff-4748-a46d-b9997d7212c6",
   "metadata": {},
   "source": [
    "### Saving LDA Model & TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94c587dd-6d4f-449e-a697-dc07cf2c82c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lda_model.pkl' saved in C:/Users/adrco/Final_Project-env/LDA_Model/lda_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Define the path for saving the LDA model\n",
    "lda_model_path = \"C:/Users/adrco/Final_Project-env/LDA_Model/lda_model.pkl\"\n",
    "\n",
    "# Save the LDA model\n",
    "joblib.dump(lda_model, lda_model_path)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"'lda_model.pkl' saved in {lda_model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c958085-86a8-4d27-86d7-60eeaa7c48a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'tfidf_vectorizer.pkl' saved in C:/Users/adrco/Final_Project-env/TF-IDF_Vectorizer/tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the path for saving the TF-IDF Vectorizer\n",
    "tfidf_vectorizer_path = \"C:/Users/adrco/Final_Project-env/TF-IDF_Vectorizer/tfidf_vectorizer.pkl\"\n",
    "\n",
    "# Save the TF-IDF Vectorizer\n",
    "joblib.dump(tfidf_vectorizer, tfidf_vectorizer_path)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"'tfidf_vectorizer.pkl' saved in {tfidf_vectorizer_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2cfea1-ba83-48a4-bfa5-5c87c69ffb4f",
   "metadata": {},
   "source": [
    "### Saving the Document-Topic Distribution Matrix\n",
    "\n",
    "The document-topic distribution matrix, which indicates the distribution of topics across our articles, is saved for easy retrieval. This matrix is fundamental for further analysis and understanding the prevalence of topics within our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e8fa31a-b3a8-4142-ac08-82df881800b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.3s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate the document-topic distribution matrix\n",
    "doc_topic_dist = lda_model.transform(tfidf_matrix)\n",
    "\n",
    "# Convert document-topic distribution matrix to DataFrame\n",
    "doc_topic_dist_df = pd.DataFrame(doc_topic_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4318e5-49e7-43bb-808c-f5563d0e18f9",
   "metadata": {},
   "source": [
    "### Append Dominant Topic to DataFrame\n",
    "\n",
    "The dominant topic is determined by identifying the topic with the highest weight for each document in the topic distribution matrix produced by the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dfbc942-3a2e-40c1-871d-1de46c943116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Identifying and appending the dominant topic for each document\n",
    "dominant_topic = np.argmax(doc_topic_dist, axis=1)\n",
    "df['Dominant_Topic'] = dominant_topic + 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c3c96-1189-40f3-9d16-104fabb4dd64",
   "metadata": {},
   "source": [
    "### Incorporate Topic Distribution Weights into DataFrame\n",
    "\n",
    "Next, we include the distribution weights of all topics for each article as new columns in the DataFrame. These weights represent the proportion of each topic within an article, providing a detailed view of the article's thematic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74892a6f-d49d-4b01-bbad-fef8bf555751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding topic distribution weights as new columns to the DataFrame\n",
    "topic_weight_columns = [f'Topic_{i + 1}_Weight' for i in range(15)]  \n",
    "topic_weights_df = pd.DataFrame(doc_topic_dist, columns=topic_weight_columns)\n",
    "enhanced_df = pd.concat([df, topic_weights_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a7826-ced8-45d8-b5d8-bb680ce91d39",
   "metadata": {},
   "source": [
    "### Save the Enhanced DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eebaa3bf-3123-437e-925c-4a17cd9510fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enhanced_df saved as 'enhanced_dataset_with_topics.csv' in C:/Users/adrco/Final_Project-env/Datasets/\n",
      "doc_topic_dist_df saved as 'doc_topic_distribution.csv' in C:/Users/adrco/Final_Project-env/Datasets/\n"
     ]
    }
   ],
   "source": [
    "# Saving the enhanced DataFrame with topic information to CSV in the specified dataset path\n",
    "enhanced_df.to_csv(dataset_path + 'enhanced_dataset_with_topics.csv', index=False)\n",
    "\n",
    "print(f\"enhanced_df saved as 'enhanced_dataset_with_topics.csv' in {dataset_path}\")\n",
    "\n",
    "# Saving the document-topic distribution matrix\n",
    "doc_topic_dist_df.to_csv(dataset_path + 'doc_topic_distribution.csv', index=False)\n",
    "\n",
    "print(f\"doc_topic_dist_df saved as 'doc_topic_distribution.csv' in {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60cc8e-cfa8-46e0-a3f9-8656462dfc95",
   "metadata": {},
   "source": [
    "The enhanced DataFrame, which now includes the dominant topic for each document along with the topic distribution weights, will be saved to our project's dataset directory. This step ensures that we have a permanent record of the topic modeling enhancements made to our dataset, allowing for easy access and further analysis in subsequent stages of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b876b-2260-496e-8c04-2d41e68527fa",
   "metadata": {},
   "source": [
    "### Documenting Topics and Their Top Words\r\n",
    "\r\n",
    "For reference and further analysis, we document the topics identified by our LDA model along with their top words. Saving this information to a text file provides a quick overview of each topic's main themes, aiding in the interpretation and communication of our topic modeling results.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71a56aac-dec4-4adf-b8e6-7ef10dcd8564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics summary saved to C:/Users/adrco/Final_Project-env/Datasets/topics_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the topics and their top words to a text file\n",
    "topics_summary_path = dataset_path + 'topics_summary.txt'  \n",
    "with open(topics_summary_path, 'w') as f:\n",
    "    for i, topic in enumerate(lda_model.components_):\n",
    "        top_features_ind = topic.argsort()[:-11:-1]\n",
    "        top_features = [tfidf_vectorizer.get_feature_names_out()[j] for j in top_features_ind]\n",
    "        # Adjust topic numbering to start from 1\n",
    "        topic_str = \"Topic {}: {}\\n\".format(i + 1, ' '.join(top_features))\n",
    "        f.write(topic_str)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Topics summary saved to {topics_summary_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073394f8-0d46-4772-a5b7-86a225205f78",
   "metadata": {},
   "source": [
    "### Mapping Numeric Topics to Descriptive Labels\n",
    "\n",
    "To enhance interpretability of the dominant topics in our dataset, we will map the numeric topic identifiers to descriptive labels. This step transforms the abstract topic numbers into meaningful labels that reflect the primary theme or subject matter of each topic, facilitating easier analysis and communication of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b5d14e-5eb8-4a4e-aa29-5a55d1112fa2",
   "metadata": {},
   "source": [
    "### Determination of Topic Names\n",
    "\n",
    "The names assigned to each topic were derived from analyzing the top representative words identified by the Latent Dirichlet Allocation (LDA) model. These names are intended to capture the essence of the themes or subject matters that are prevalent within each topic, based on the clustering of similar words. Here is how we labeled each topic from 1 to 15, with a brief description of their focus areas:\n",
    "\n",
    "- **Topic 1 - Banking and Legal Aspects**: Focuses on banks, legal claims, and regulatory statements in the crypto space.\n",
    "- **Topic 2 - Bitcoin Mining**: Covers aspects of Bitcoin mining, including miner activity and transaction details.\n",
    "- **Topic 3 - Business and Taxation**: Related to business operations, taxation, and public sector involvement in crypto.\n",
    "- **Topic 4 - Investment and Digital Assets**: Discusses company investments in digital assets, including asset management and services.\n",
    "- **Topic 5 - Market Dynamics**: Captures short-term market movements, including daily price changes and trading volume.\n",
    "- **Topic 6 - Economic Indicators**: Concerns with macroeconomic indicators like inflation rates and federal reserve policies.\n",
    "- **Topic 7 - Technical Analysis**: Deals with technical analysis indicators like support/resistance levels and price trends.\n",
    "- **Topic 8 - Government and Security**: Focuses on government actions, security issues, and regulatory responses to crypto.\n",
    "- **Topic 9 - Market Capitalization and Trading**: Involves market cap, trading volumes, and price predictions for cryptocurrencies.\n",
    "- **Topic 10 - Exchanges and Crises**: Pertains to crypto exchanges, notable crises like the FTX collapse, and their implications.\n",
    "- **Topic 11 - Market Sentiment**: Reflects on overall market sentiment, including bull/bear market indicators and volatility.\n",
    "- **Topic 12 - Price Comparisons**: Discusses Bitcoin's price movements in relation to other assets like gold.\n",
    "- **Topic 13 - Token Sales and Funding**: Covers topics related to presales, token fundraising, and initial coin offerings (ICOs).\n",
    "- **Topic 14 - Blockchain Technology**: Focuses on blockchain technology developments, user engagement, and future prospects.\n",
    "- **Topic 15 - Trading Platforms**: Relates to the use of trading platforms, social trading, and decision-making tools for traders.\n",
    "\n",
    "This mapping from topics to descriptive labels enhances the interpretability of our dataset, facilitating clearer communication of findings and supporting more nuanced analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0fe69fd-dda8-4ccb-ae56-e197f56b5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of topic numbers to descriptive labels \n",
    "topic_labels = {\n",
    "    1: \"Banking and Legal Aspects\",\n",
    "    2: \"Bitcoin Mining\",\n",
    "    3: \"Business and Taxation\",\n",
    "    4: \"Investment and Digital Assets\",\n",
    "    5: \"Market Dynamics\",\n",
    "    6: \"Economic Indicators\",\n",
    "    7: \"Technical Analysis\",\n",
    "    8: \"Government and Security\",\n",
    "    9: \"Market Capitalization and Trading\",\n",
    "    10: \"Exchanges and Crises\",\n",
    "    11: \"Market Sentiment\",\n",
    "    12: \"Price Comparisons\",\n",
    "    13: \"Token Sales and Funding\",\n",
    "    14: \"Blockchain Technology\",\n",
    "    15: \"Trading Platforms\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to replace numeric topic identifiers with descriptive labels\n",
    "enhanced_df['Dominant_Topic_Label'] = enhanced_df['Dominant_Topic'].map(topic_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b421d3f-f130-4950-85b3-e16e0a52a3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced dataset with adjusted topic numbering and descriptive labels saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Saving the updated enhanced DataFrame with adjusted topic numbering and descriptive labels\n",
    "enhanced_df.to_csv(dataset_path + 'enhanced_dataset_with_topics.csv', index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Enhanced dataset with adjusted topic numbering and descriptive labels saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d9888-7eec-4322-a80c-7f00f984a1f7",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with FinBERT\r\n",
    "\r\n",
    "In this section, we will perform sentiment analysis on the \"processed_text\" column of our dataset using FinBERT. FinBERT is a pre-trained NLP model specialized for financial texts. This analysis will help us understand the overall sentiment (positive, neutral, negative) conveyed in cryptocurrency news articles. The sentiment scores will be stored in a new column named \"finBERT_sentiment_score\", with values -1 (negative), 0 (neutral), and 1 (positive).\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2e2205c-268e-40dc-b4d9-5adace9db7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     datetime                                               text  \\\n",
      "0  2022-10-14  despite fact blockchainbased carbon credit mar...   \n",
      "1  2022-10-14  trader gained huge kudos space predicting drop...   \n",
      "2  2022-10-14  always worked sticking plan clear invalidation...   \n",
      "3  2022-10-14  fact broke level system giving bullish signals...   \n",
      "4  2022-10-14  demand coming confirms theres fuel keep going ...   \n",
      "\n",
      "                                                 url  price_momentums  \\\n",
      "0  https://cryptonews.com/news/bitcoin-price-and-...                1   \n",
      "1  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "2  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "3  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "4  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "\n",
      "                                      processed_text  Dominant_Topic  \\\n",
      "0  despite fact blockchainbased carbon credit mar...               2   \n",
      "1  trader gained huge kudos space predicting drop...               2   \n",
      "2  always worked sticking plan clear invalidation...               7   \n",
      "3  fact broke level system giving bullish signals...              11   \n",
      "4  demand coming confirms theres fuel keep going ...              11   \n",
      "\n",
      "   Topic_1_Weight  Topic_2_Weight  Topic_3_Weight  Topic_4_Weight  ...  \\\n",
      "0        0.016250        0.541053        0.016250        0.016250  ...   \n",
      "1        0.016348        0.600144        0.016348        0.016348  ...   \n",
      "2        0.018534        0.018534        0.018534        0.018534  ...   \n",
      "3        0.016822        0.016822        0.016822        0.016822  ...   \n",
      "4        0.127245        0.017505        0.017505        0.017505  ...   \n",
      "\n",
      "   Topic_7_Weight  Topic_8_Weight  Topic_9_Weight  Topic_10_Weight  \\\n",
      "0        0.016250        0.016250        0.016250         0.016250   \n",
      "1        0.016348        0.016348        0.016348         0.016348   \n",
      "2        0.325636        0.018534        0.138005         0.018535   \n",
      "3        0.316788        0.016822        0.016822         0.016822   \n",
      "4        0.017505        0.017505        0.017505         0.017505   \n",
      "\n",
      "   Topic_11_Weight  Topic_12_Weight  Topic_13_Weight  Topic_14_Weight  \\\n",
      "0         0.016250         0.016250         0.016250         0.016250   \n",
      "1         0.016348         0.016348         0.016348         0.016348   \n",
      "2         0.018534         0.313945         0.018534         0.018534   \n",
      "3         0.464527         0.016822         0.016822         0.016822   \n",
      "4         0.489102         0.173597         0.017505         0.017505   \n",
      "\n",
      "   Topic_15_Weight  Dominant_Topic_Label  \n",
      "0         0.016250        Bitcoin Mining  \n",
      "1         0.016348        Bitcoin Mining  \n",
      "2         0.018534    Technical Analysis  \n",
      "3         0.016822      Market Sentiment  \n",
      "4         0.017505      Market Sentiment  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = \"C:/Users/adrco/Final_Project-env/Datasets/enhanced_dataset_with_topics.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "enhanced_df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows to ensure it's loaded correctly\n",
    "print(enhanced_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5138341-69e0-41fb-8d1b-7acbd70e6ee6",
   "metadata": {},
   "source": [
    "### Initializing FinBERT\n",
    "\n",
    "Before analyzing the sentiments of our dataset, we need to initialize the FinBERT model and tokenizer. These components are crucial for preparing our text data (tokenization) and for performing the sentiment analysis through the model's pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5b961d5-b191-49bb-83c7-1a98e622cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model for FinBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# Initialize the sentiment analysis pipeline\n",
    "finbert_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00906b59-950f-4ba9-9ce7-1c7a2a116ef3",
   "metadata": {},
   "source": [
    "### Performing Sentiment Analysis\r\n",
    "\r\n",
    "We will now apply the FinBERT model to analyze sentiments of the processed texts. The sentiment output from FinBERT will be mapped to numerical scores: -1 for negative, 0 for neutral, and 1 for positive sentiments. This process enriches our dataset with valuable sentiment insights directly relevant to our analysis of cryptocurrency market dynamics.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1797cff1-39e8-4996-bc79-481773987c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d48165f40948acb403ab28f1434ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Sentiments:   0%|          | 0/8113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     datetime                                               text  \\\n",
      "0  2022-10-14  despite fact blockchainbased carbon credit mar...   \n",
      "1  2022-10-14  trader gained huge kudos space predicting drop...   \n",
      "2  2022-10-14  always worked sticking plan clear invalidation...   \n",
      "3  2022-10-14  fact broke level system giving bullish signals...   \n",
      "4  2022-10-14  demand coming confirms theres fuel keep going ...   \n",
      "\n",
      "                                                 url  price_momentums  \\\n",
      "0  https://cryptonews.com/news/bitcoin-price-and-...                1   \n",
      "1  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "2  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "3  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "4  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "\n",
      "                                      processed_text  Dominant_Topic  \\\n",
      "0  despite fact blockchainbased carbon credit mar...               2   \n",
      "1  trader gained huge kudos space predicting drop...               2   \n",
      "2  always worked sticking plan clear invalidation...               7   \n",
      "3  fact broke level system giving bullish signals...              11   \n",
      "4  demand coming confirms theres fuel keep going ...              11   \n",
      "\n",
      "   Topic_1_Weight  Topic_2_Weight  Topic_3_Weight  Topic_4_Weight  ...  \\\n",
      "0        0.016250        0.541053        0.016250        0.016250  ...   \n",
      "1        0.016348        0.600144        0.016348        0.016348  ...   \n",
      "2        0.018534        0.018534        0.018534        0.018534  ...   \n",
      "3        0.016822        0.016822        0.016822        0.016822  ...   \n",
      "4        0.127245        0.017505        0.017505        0.017505  ...   \n",
      "\n",
      "   Topic_8_Weight  Topic_9_Weight  Topic_10_Weight  Topic_11_Weight  \\\n",
      "0        0.016250        0.016250         0.016250         0.016250   \n",
      "1        0.016348        0.016348         0.016348         0.016348   \n",
      "2        0.018534        0.138005         0.018535         0.018534   \n",
      "3        0.016822        0.016822         0.016822         0.464527   \n",
      "4        0.017505        0.017505         0.017505         0.489102   \n",
      "\n",
      "   Topic_12_Weight  Topic_13_Weight  Topic_14_Weight  Topic_15_Weight  \\\n",
      "0         0.016250         0.016250         0.016250         0.016250   \n",
      "1         0.016348         0.016348         0.016348         0.016348   \n",
      "2         0.313945         0.018534         0.018534         0.018534   \n",
      "3         0.016822         0.016822         0.016822         0.016822   \n",
      "4         0.173597         0.017505         0.017505         0.017505   \n",
      "\n",
      "   Dominant_Topic_Label  finBERT_sentiment_score  \n",
      "0        Bitcoin Mining                        1  \n",
      "1        Bitcoin Mining                       -1  \n",
      "2    Technical Analysis                        0  \n",
      "3      Market Sentiment                        0  \n",
      "4      Market Sentiment                       -1  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define a function to apply FinBERT sentiment analysis and map to numerical scores\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        result = finbert_pipeline(text)[0]\n",
    "        # Correctly map the labels to numerical scores\n",
    "        sentiment_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "        return sentiment_mapping[result['label']]\n",
    "    except Exception as e:\n",
    "        # Print the error and the text for better debugging\n",
    "        print(f\"Error analyzing text: {text}\\nException: {e}\")\n",
    "        # Return None in case of an error\n",
    "        return None\n",
    "\n",
    "# Apply sentiment analysis to the 'processed_text' column with progress bar\n",
    "tqdm.pandas(desc=\"Analyzing Sentiments\")\n",
    "enhanced_df['finBERT_sentiment_score'] = enhanced_df['processed_text'].progress_apply(analyze_sentiment)\n",
    "\n",
    "# Display the first few rows to verify the sentiment scores\n",
    "print(enhanced_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29328074-6fab-4741-8c75-745b42b87b9a",
   "metadata": {},
   "source": [
    "# Aspect-Based Sentiment Analysis\r\n",
    "\r\n",
    "This section focuses on extracting aspects from the cryptocurrency news articles and analyzing the sentiment of each aspect using FinBERT. Aspect-based sentiment analysis allows us to understand the sentiment towards specific entities or topics within the text, providing deeper insights into the dataset. We will use spaCy for aspect extraction and then apply FinBERT to assess the sentiment of sentences or fragments containing those aspects.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26e392aa-c184-4d5f-85d6-8ff78605d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Initialize tokenizer and model for FinBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# Initialize the sentiment analysis pipeline with FinBERT\n",
    "finbert_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675c1e13-2e6d-4bac-a6ae-e3f5b85754ac",
   "metadata": {},
   "source": [
    "### Defining the Aspect Sentiment Analysis Function\r\n",
    "\r\n",
    "We will define a function to perform two main tasks: extract aspects using spaCy and analyze the sentiment of these aspects using FinBERT. The function will process each text, identify nouns and proper nouns as aspects, and determine the sentiment of the text related to each aspect. The output will be a list of tuples, each containing an aspect and its associated sentiment score.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9af09546-1ed2-4755-96ad-0d8d4562824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3e3432a0b04267b2d033404724c919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting Aspect Sentiments:   0%|          | 0/8113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_aspect_sentiments(text):\n",
    "    # Extract aspects from the text\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    aspects = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
    "    \n",
    "    # Initialize a list to hold the sentiment scores for each aspect\n",
    "    aspect_sentiments = []\n",
    "    \n",
    "    # Analyze sentiment for each aspect\n",
    "    for aspect in aspects:\n",
    "        # Replace the aspect in the text with a placeholder for targeted sentiment analysis\n",
    "        modified_text = text.replace(aspect, \"<aspect>\")\n",
    "        sentiment_result = finbert_pipeline(modified_text)[0]\n",
    "        \n",
    "        # Map the sentiment label to a numerical score\n",
    "        sentiment_score = {'positive': 1, 'neutral': 0, 'negative': -1}[sentiment_result['label']]\n",
    "        \n",
    "        # Append the aspect and its sentiment score to the list\n",
    "        aspect_sentiments.append((aspect, sentiment_score))\n",
    "    \n",
    "    return aspect_sentiments\n",
    "\n",
    "# Apply the function to each row in the dataset\n",
    "tqdm.pandas(desc=\"Extracting Aspect Sentiments\")\n",
    "enhanced_df['aspect_sentiments'] = enhanced_df['processed_text'].progress_apply(extract_aspect_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "644a5743-a960-46ea-881f-7d1967de6e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     datetime                                               text  \\\n",
      "0  2022-10-14  despite fact blockchainbased carbon credit mar...   \n",
      "1  2022-10-14  trader gained huge kudos space predicting drop...   \n",
      "2  2022-10-14  always worked sticking plan clear invalidation...   \n",
      "3  2022-10-14  fact broke level system giving bullish signals...   \n",
      "4  2022-10-14  demand coming confirms theres fuel keep going ...   \n",
      "\n",
      "                                                 url  price_momentums  \\\n",
      "0  https://cryptonews.com/news/bitcoin-price-and-...                1   \n",
      "1  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "2  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "3  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "4  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "\n",
      "                                      processed_text  Dominant_Topic  \\\n",
      "0  despite fact blockchainbased carbon credit mar...               2   \n",
      "1  trader gained huge kudos space predicting drop...               2   \n",
      "2  always worked sticking plan clear invalidation...               7   \n",
      "3  fact broke level system giving bullish signals...              11   \n",
      "4  demand coming confirms theres fuel keep going ...              11   \n",
      "\n",
      "   Topic_1_Weight  Topic_2_Weight  Topic_3_Weight  Topic_4_Weight  ...  \\\n",
      "0        0.016250        0.541053        0.016250        0.016250  ...   \n",
      "1        0.016348        0.600144        0.016348        0.016348  ...   \n",
      "2        0.018534        0.018534        0.018534        0.018534  ...   \n",
      "3        0.016822        0.016822        0.016822        0.016822  ...   \n",
      "4        0.127245        0.017505        0.017505        0.017505  ...   \n",
      "\n",
      "   Topic_9_Weight  Topic_10_Weight  Topic_11_Weight  Topic_12_Weight  \\\n",
      "0        0.016250         0.016250         0.016250         0.016250   \n",
      "1        0.016348         0.016348         0.016348         0.016348   \n",
      "2        0.138005         0.018535         0.018534         0.313945   \n",
      "3        0.016822         0.016822         0.464527         0.016822   \n",
      "4        0.017505         0.017505         0.489102         0.173597   \n",
      "\n",
      "   Topic_13_Weight  Topic_14_Weight  Topic_15_Weight  Dominant_Topic_Label  \\\n",
      "0         0.016250         0.016250         0.016250        Bitcoin Mining   \n",
      "1         0.016348         0.016348         0.016348        Bitcoin Mining   \n",
      "2         0.018534         0.018534         0.018534    Technical Analysis   \n",
      "3         0.016822         0.016822         0.016822      Market Sentiment   \n",
      "4         0.017505         0.017505         0.017505      Market Sentiment   \n",
      "\n",
      "   finBERT_sentiment_score                                  aspect_sentiments  \n",
      "0                        1  [(fact, -1), (carbon, -1), (credit, -1), (mark...  \n",
      "1                       -1  [(trader, -1), (kudos, -1), (space, -1), (drop...  \n",
      "2                        0  [(plan, 0), (invalidation, 0), (invalidation, ...  \n",
      "3                        0  [(fact, 1), (level, 1), (system, 1), (signals,...  \n",
      "4                       -1  [(demand, -1), (fuel, -1), (squeeze, -1), (21k...  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows to verify \n",
    "print(enhanced_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "761df7bc-20f7-423a-b7cc-9ca4ede6685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced dataset saved as 'enhanced_dataset.csv' in C:/Users/adrco/Final_Project-env/Datasets/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"C:/Users/adrco/Final_Project-env/Datasets/\"  # Re-asserting path to avoid reloading all previous notebooks\n",
    "\n",
    "# Directly save the DataFrame with the new filename\n",
    "enhanced_df.to_csv(dataset_path + \"enhanced_dataset.csv\", index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Enhanced dataset saved as 'enhanced_dataset.csv' in {dataset_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef56962-207c-4f2d-ae2c-8c2f5c69d41f",
   "metadata": {},
   "source": [
    "## Generating Word Embeddings with Word2Vec\r\n",
    "\r\n",
    "Word embeddings provide a way to represent text data in numerical form, capturing the context and semantic relationships between words. By using Word2Vec, we can convert the processed text from our articles into vectors that encapsulate these relationships. These vectors can then be used as features in machine learning models, potentially improving our ability to predict cryptocurrency price momentums based on the content of news articles. In this section, we will train a Word2Vec model on our dataset and explore how to utilize these embeddings as features for our predictive modeling tasks.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e78969-9ded-43dd-b63c-50c6b9603c2d",
   "metadata": {},
   "source": [
    "### Optimizing Word2Vec Parameters for Cryptocurrency News\n",
    "\n",
    "To enhance the quality of word embeddings generated from our cryptocurrency news articles dataset, we've carefully selected a set of Word2Vec parameters tailored to the characteristics of our text data. These optimized parameters aim to capture the nuanced semantic relationships and specific vocabulary present in the financial domain, particularly in cryptocurrency news. Here's a brief overview of the chosen parameters and their significance:\n",
    "\n",
    "- **`vector_size=300`:** We increased the dimensionality of the word vectors to 300. This higher dimensionality allows the model to capture a richer set of semantic relationships and nuances in the text, which is particularly beneficial for the specialized vocabulary of cryptocurrency news.\n",
    "\n",
    "- **`window=10`:** The window size was set to 10, allowing the model to consider a broader context when predicting words. This is crucial for capturing the complex relationships between terms that are further apart in the text, enhancing the embeddings' ability to reflect the context-dependent meanings of words in financial news.\n",
    "\n",
    "- **`min_count=2`:** We set the minimum word frequency threshold to 2, ignoring words that appear only once in the dataset. This helps focus the model on more relevant and recurrent vocabulary, reducing noise from very rare terms and improving the overall quality of the embeddings.\n",
    "\n",
    "- **`sg=1`:** We chose the Skip-gram architecture over CBOW because Skip-gram is generally more effective for datasets with specialized vocabularies and rare words, common in cryptocurrency news. Skip-gram focuses on predicting context words from target words, which helps in learning high-quality embeddings for less frequent terms.\n",
    "\n",
    "- **`workers=4`:** The number of worker threads was set to 4 to leverage multi-core processing, speeding up the training process. Adjust this number based on your machine's CPU cores to optimize training time without compromising model quality.\n",
    "\n",
    "By adjusting these parameters, our Word2Vec model is better equipped to generate meaningful and informative word embeddings from our cryptocurrency news dataset, providing a solid foundation for subsequent predictive modeling tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6818dd65-b2d0-4ef6-9dcf-a4813eaf1a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adrco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=8084, vector_size=300, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure NLTK's tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Since our 'processed_text' column in 'enhanced_df' contains plain string representations of texts and not lists of tokens,\n",
    "# we need to tokenize these strings into list of words.\n",
    "# Preparing our dataset for training the Word2Vec model.\n",
    "enhanced_df['tokenized_text'] = enhanced_df['processed_text'].apply(word_tokenize)\n",
    "\n",
    "# Training the Word2Vec model with optimized parameters. This model will learn word embeddings from our tokenized texts,\n",
    "# capturing the semantic relationships between words based on their co-occurrences in the dataset.\n",
    "model = Word2Vec(sentences=enhanced_df['tokenized_text'], vector_size=300, window=10, min_count=2, sg=1, workers=4)\n",
    "\n",
    "# Summarize the loaded model to verify its configuration and the vocabulary size.\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01cd0c1-c9ac-4dab-a6fa-92d8cea6405a",
   "metadata": {},
   "source": [
    "### Interpreting the Word2Vec Model Summary\r\n",
    "\r\n",
    "The output of our Word2Vec model training provides a concise summary of the model's characteristics. Specifically, the summary `Word2Vec<vocab=8084, vector_size=300, alpha=0.025>` indicates several key aspects of the trained model:\r\n",
    "\r\n",
    "- **`vocab=8084`**: This number represents the size of the model's vocabulary. It means that our Word2Vec model has learned embeddings for 8,084 unique words found in the `processed_text` column of our dataset. This vocabulary size is a reflection of the diversity and richness of the textual content we're analyzing.\r\n",
    "\r\n",
    "- **`vector_size=300`**: This parameter indicates the dimensionality of the word vectors generated by the model. Each word in the model's vocabulary is represented as a 300-dimensional vector. A higher dimensionality allows the model to capture more nuanced semantic relationships between words, though it also increases the model's complexity and memory requirements.\r\n",
    "\r\n",
    "- **`alpha=0.025`**: This is the initial learning rate for the training algorithm. The learning rate controls how much the model's weights are adjusted during training with respect to the loss gradient. Over time, this rate can decrease. A typical starting value is 0.025, and it is automatically tuned during training based on the model's performance and optimization settings.\r\n",
    "\r\n",
    "In summary, the model's output gives us insight into the scale of our text data (through the vocabulary size), the complexity of the word embeddings (via the vector size), and a glimpse into the training process (through the initial learning rate). With this model, we're now equipped to explore the semantic space of cryptocurrency news articles, leveraging these embeddings for downstream tasks such as sentiment analysis, clustering, or predictive modeling.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5377789-6b54-4f0d-b146-a36f2c57c651",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6f3916b-8af0-4f93-854f-9e7970c06ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model saved to C:\\Users\\adrco\\Final_Project-env\\Word2Vec_Model\\word2vec_model.model\n"
     ]
    }
   ],
   "source": [
    "# Define the path where you want to save the Word2Vec model\n",
    "model_save_path = \"C:\\\\Users\\\\adrco\\\\Final_Project-env\\\\Word2Vec_Model\\\\word2vec_model.model\"\n",
    "\n",
    "# Saving the model\n",
    "model.save(model_save_path)\n",
    "\n",
    "# Print confirmation \n",
    "print(f\"Word2Vec model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e7d4d-6167-4985-ba45-0ac52e782900",
   "metadata": {},
   "source": [
    "## Preparing Word Embeddings for Machine Learning\r\n",
    "\r\n",
    "Having trained our Word2Vec model on the cryptocurrency news articles, we now possess a rich set of word embeddings that capture the semantic nuances of the financial domain. Each word in our dataset is represented as a 300-dimensional vector, encapsulating its context and relationships with other words.\r\n",
    "\r\n",
    "The next step involves leveraging these embeddings to represent entire articles, not just individual words, in a format suitable for machine learning models. Since models require fixed-size input vectors, we cannot directly feed them lists of varying-length word vectors. To address this, we'll aggregate word vectors within an article to create a single, fixed-size vector representation for the entire text.\r\n",
    "\r\n",
    "### Average Word Vectors\r\n",
    "\r\n",
    "A straightforward and effective approach to achieve this is by calculating the average word vector for each article. This method involves summing the vectors of all words in an article and dividing by the number of words, resulting in a single vector that represents the semantic essence of the text. This average vector can then be used as a feature set for predictive modeling tasks, such as forecasting cryptocurrency price movements based on news content.\r\n",
    "\r\n",
    "In the following code cell, we define a function that computes the average word vector for a list of words (tokens) from our processed text. This function ensures that only words present in our Word2Vec model's vocabulary are considered, avoiding any errors due to out-of-vocabulary words.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "905998f3-9337-4b1b-98de-70f971a885f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, vector_size):\n",
    "    feature_vec = np.zeros((vector_size,), dtype=\"float32\")  # Initialize a zero vector of the same dimensionality as Word2Vec vectors\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:  # Check if the word is in the Word2Vec model's vocabulary\n",
    "            nwords += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[word])  # Add the word's vector to the feature vector\n",
    "    \n",
    "    if nwords > 0:\n",
    "        feature_vec = np.divide(feature_vec, nwords)  # Average the feature vector by the number of words\n",
    "    return feature_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9c2ce-3a79-489e-bb53-d2d6109fcf84",
   "metadata": {},
   "source": [
    "### Applying Word Embeddings to the Dataset\n",
    "\n",
    "With our Word2Vec model trained and the vocabulary set prepared, we're now ready to transform our text data into a format that captures the semantic richness encoded in the word embeddings. By applying the average word vectors function to each article in our dataset, we consolidate the vast information contained in individual word vectors into a single, cohesive vector representation for each article. This process enables us to represent the entire textual content of an article with a fixed-size vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "190c085b-0a4a-4c26-959f-124efee91240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the model's vocabulary. It's important for filtering words when calculating averages.\n",
    "vocabulary = set(model.wv.index_to_key)\n",
    "\n",
    "# Vector size used in Word2Vec model\n",
    "vector_size = 300\n",
    "\n",
    "# Apply the function to each row in the DataFrame. This will create a new column with the averaged word vectors.\n",
    "enhanced_df['word_vector'] = enhanced_df['tokenized_text'].apply(lambda x: average_word_vectors(x, model, vocabulary, vector_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e322e-a721-49a4-b24b-2bfa21257f59",
   "metadata": {},
   "source": [
    "### Saving the Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fc13b1f-3c18-49bf-8ccf-9e650ae6538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced DataFrame with Word2Vec features saved to C:/Users/adrco/Final_Project-env/Datasets/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Saving the enhanced DataFrame with Word2Vec features\n",
    "enhanced_df.to_pickle(dataset_path + \"enhanced_df_with_word_vectors.pkl\")  # Using pickle to preserve the vector format\n",
    "\n",
    "# Print confirmation \n",
    "print(f\"Enhanced DataFrame with Word2Vec features saved to {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce2c16-dcfa-487a-b6a2-117888438f21",
   "metadata": {},
   "source": [
    "### Verifying the Enhanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cca735ce-123e-49dd-9f04-8bed95ded629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     datetime                                               text  \\\n",
      "0  2022-10-14  despite fact blockchainbased carbon credit mar...   \n",
      "1  2022-10-14  trader gained huge kudos space predicting drop...   \n",
      "2  2022-10-14  always worked sticking plan clear invalidation...   \n",
      "3  2022-10-14  fact broke level system giving bullish signals...   \n",
      "4  2022-10-14  demand coming confirms theres fuel keep going ...   \n",
      "\n",
      "                                                 url  price_momentums  \\\n",
      "0  https://cryptonews.com/news/bitcoin-price-and-...                1   \n",
      "1  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "2  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "3  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "4  https://cryptonews.com/news/bitcoin-price-pred...                1   \n",
      "\n",
      "                                      processed_text  Dominant_Topic  \\\n",
      "0  despite fact blockchainbased carbon credit mar...               2   \n",
      "1  trader gained huge kudos space predicting drop...               2   \n",
      "2  always worked sticking plan clear invalidation...               7   \n",
      "3  fact broke level system giving bullish signals...              11   \n",
      "4  demand coming confirms theres fuel keep going ...              11   \n",
      "\n",
      "   Topic_1_Weight  Topic_2_Weight  Topic_3_Weight  Topic_4_Weight  ...  \\\n",
      "0        0.016250        0.541053        0.016250        0.016250  ...   \n",
      "1        0.016348        0.600144        0.016348        0.016348  ...   \n",
      "2        0.018534        0.018534        0.018534        0.018534  ...   \n",
      "3        0.016822        0.016822        0.016822        0.016822  ...   \n",
      "4        0.127245        0.017505        0.017505        0.017505  ...   \n",
      "\n",
      "   Topic_11_Weight  Topic_12_Weight  Topic_13_Weight  Topic_14_Weight  \\\n",
      "0         0.016250         0.016250         0.016250         0.016250   \n",
      "1         0.016348         0.016348         0.016348         0.016348   \n",
      "2         0.018534         0.313945         0.018534         0.018534   \n",
      "3         0.464527         0.016822         0.016822         0.016822   \n",
      "4         0.489102         0.173597         0.017505         0.017505   \n",
      "\n",
      "   Topic_15_Weight  Dominant_Topic_Label  finBERT_sentiment_score  \\\n",
      "0         0.016250        Bitcoin Mining                        1   \n",
      "1         0.016348        Bitcoin Mining                       -1   \n",
      "2         0.018534    Technical Analysis                        0   \n",
      "3         0.016822      Market Sentiment                        0   \n",
      "4         0.017505      Market Sentiment                       -1   \n",
      "\n",
      "                                   aspect_sentiments  \\\n",
      "0  [(fact, -1), (carbon, -1), (credit, -1), (mark...   \n",
      "1  [(trader, -1), (kudos, -1), (space, -1), (drop...   \n",
      "2  [(plan, 0), (invalidation, 0), (invalidation, ...   \n",
      "3  [(fact, 1), (level, 1), (system, 1), (signals,...   \n",
      "4  [(demand, -1), (fuel, -1), (squeeze, -1), (21k...   \n",
      "\n",
      "                                      tokenized_text  \\\n",
      "0  [despite, fact, blockchainbased, carbon, credi...   \n",
      "1  [trader, gained, huge, kudos, space, predictin...   \n",
      "2  [always, worked, sticking, plan, clear, invali...   \n",
      "3  [fact, broke, level, system, giving, bullish, ...   \n",
      "4  [demand, coming, confirms, theres, fuel, keep,...   \n",
      "\n",
      "                                         word_vector  \n",
      "0  [0.074189804, 0.08830079, -0.01618443, 0.04870...  \n",
      "1  [0.10852058, 0.07451202, -0.031024793, 0.10725...  \n",
      "2  [0.07917403, 0.055064432, -0.0067611514, 0.098...  \n",
      "3  [0.16005856, 0.03238435, -0.056046773, 0.13209...  \n",
      "4  [0.097828545, 0.07860845, 0.008878434, 0.11065...  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load a small sample to verify the word vector column\n",
    "print(enhanced_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe74f8-d034-4476-8035-76623ec9ca78",
   "metadata": {},
   "source": [
    "### Analyzing the Enhanced DataFrame with Word2Vec Features\n",
    "\n",
    "- **`word_vector`:** This new column contains the average Word2Vec vector for each article. Each entry is a list of numerical values, each representing a dimension in the 300-dimensional vector space where the Word2Vec model maps words. These vectors encapsulate the semantic essence of the article's text, making them valuable features for machine learning models.\n",
    "\n",
    "By adding these word vectors to our dataset, we've enriched it with quantitative features that capture the semantic properties of the articles' text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ea319-93f2-49de-bf39-d72944b55ffe",
   "metadata": {},
   "source": [
    "## Conclusion on Feature Engineering\n",
    "\n",
    "The feature engineering stage of our project is complete. We have added several important features to our dataset, including FinBERT sentiment scores, aspect-based sentiments, Word2Vec embeddings, identified topics, and their corresponding weights. These features are intended to capture the detailed content and sentiment of cryptocurrency news articles, which are vital for our subsequent analysis.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "The next phase involves feature selection and comparative analysis, which will be conducted in a new notebook titled `ML_CryptoNews_FS`. Our aim is to identify the features that most effectively predict our target variable, \"price momentums\". We will use various feature selection techniques and tools to examine the relationship between our engineered features and the target variable. This step is crucial for optimizing our predictive models to accurately forecast price momentum based on news article content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
